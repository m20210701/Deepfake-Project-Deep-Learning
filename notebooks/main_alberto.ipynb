{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\Alberto Parenti\\Downloads\\STUDY\\NOVA IMS\\DEEP LEARNING\\PROJECT\\crop_part1\"\n",
    "\n",
    "#We need a way to convert our training set into the correct format\n",
    "\n",
    "def load_real_samples(image_path):\n",
    "    \n",
    "    #Defining master array\n",
    "    master_list = list()\n",
    "\n",
    "    for image in os.listdir(image_path):\n",
    "        #loading image\n",
    "        img = load_img(os.path.join(image_path,image))\n",
    "        # convert to numpy array\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        #img_array = tf.image.resize(img_array, [28, 28])\n",
    "        #Standardizing to float\n",
    "        img_array = img_array.astype(\"float32\")\n",
    "        #Getting value between 0 and 1\n",
    "        img_array/=255.0\n",
    "\n",
    "        master_list.append(img_array)\n",
    "\n",
    "    return np.array(master_list)\n",
    "\n",
    "array_data = load_real_samples(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 200, 200, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining image shape\n",
    "\n",
    "img_rows = 200\n",
    "img_cols = 200\n",
    "channels = 3\n",
    "\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(seed_size, channels):\n",
    "    noise_shape = (100) #Defining the latent vector of size 100\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(5*5*256,activation=\"relu\",input_dim=seed_size))\n",
    "    model.add(Reshape((5,5,256)))\n",
    "    \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    GENERATE_RES = 5\n",
    "    if GENERATE_RES>1:\n",
    "      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
    "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "      model.add(BatchNormalization(momentum=0.8))\n",
    "      model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Final CNN layer\n",
    "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    #Transforming the latent space vector into an tf input\n",
    "    noise = Input(shape = noise_shape)\n",
    "    \n",
    "    #Feeding that latent space into the model to create an image\n",
    "    img = model(noise) #Generated image\n",
    "    # img = tf.reshape(img, shape=[200, 200, 3])\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the descriminator model\n",
    "def build_discriminator():\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, \n",
    "                     padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    #Converting the image into a tf input and passing that to the model\n",
    "    img = Input(shape=img_shape)\n",
    "    #Retrieving the output prediction from the model\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity) #The validity is whether or not the model thinks the image is real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, batch_size=128, save_interval=500):\n",
    "    \n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    disc_accuracy_list = []\n",
    "    disc_loss_list = []\n",
    "    generator_loss_list = []\n",
    "    epoch_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Discriminator Training\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch of real images from our array data stored in array_data\n",
    "\n",
    "        #Storing the indexes of those images in idx\n",
    "        indx = np.random.randint(0, array_data.shape[0], half_batch)\n",
    "\n",
    "        #Retreiving those indexes from the array\n",
    "        imgs = array_data[indx]\n",
    "\n",
    "        #Generating a half batch worth of latent space vectors which will comprise half of what\n",
    "        #we feed to the discriminator\n",
    "        latent_space = np.random.normal(0, 1, (half_batch, 100))\n",
    "        \n",
    "        # Generate a half batch of fake images from the half batch of latent spaces\n",
    "        generated_imgs = generator.predict(latent_space)\n",
    "\n",
    "        # Training the discriminator on real and fake images but not at the same time, always holding one constant\n",
    "        #first on the real\n",
    "        disc_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "\n",
    "        #then on the fake generated images\n",
    "        disc_loss_fake = discriminator.train_on_batch(generated_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "        #take average loss from real and fake images. \n",
    "        disc_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake) \n",
    "\n",
    "        #Now within the same for loop we train our Generator model by setting the input latent_space and\n",
    "        #ultimately training the Generator to have the Discriminator label its samples as valid or false.\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        #Create latent_space vectors as input for generator. \n",
    "        #Create as many latent_space vectors as defined by the batch size. \n",
    "        latent_space = np.random.normal(0, 1, (batch_size, 100)) \n",
    "\n",
    "        #Creating a vector of ones in order to trick the discriminator into thinking that the image is real\n",
    "        valid_y = np.array([1] * batch_size) #Creates an array of all ones of size=batch size\n",
    "\n",
    "        # Generator is part of the combined model where it paired with the discriminator\n",
    "        # Train the generator with latent_space as x and 1 as y. \n",
    "        generator_loss = combined.train_on_batch(latent_space, valid_y)\n",
    "\n",
    "\n",
    "        #In order for us to keep track of the epochs elapsed and the results we print the following\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, disc_loss[0], 100*disc_loss[1], generator_loss))\n",
    "\n",
    "        #Appending the loss and accuracy metrics to lists\n",
    "        epoch_list.append(epoch)\n",
    "        disc_loss_list.append(disc_loss[0])\n",
    "        disc_accuracy_list.append(100*disc_loss[1])\n",
    "        generator_loss_list.append(generator_loss)\n",
    "\n",
    "        # If we are at the specified interval save generated image of samples\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "    #At the end of the epochs plot the discriminator loss per epoch\n",
    "    plt.plot(epoch_list, disc_loss_list)\n",
    "    plt.show()\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    latent_space = np.random.normal(0, 1, (r * c, 100))\n",
    "    generated_imgs = generator.predict(latent_space)\n",
    "\n",
    "    # Scaling our image\n",
    "    generated_imgs = 0.5 * generated_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(generated_imgs[cnt, :,:,:])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images_conv2d/generation_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 100, 32)      896       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 100, 100, 32)      0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 100, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 51, 51, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 51, 51, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 51, 51, 64)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 51, 51, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 26, 26, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 26, 26, 256)       0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 26, 26, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 26, 26, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 26, 26, 512)       0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 26, 26, 512)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 346112)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 346113    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,918,529\n",
      "Trainable params: 1,916,609\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 6400)              646400    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 10, 10, 256)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 10, 10, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 10, 10, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10, 10, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 20, 20, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 20, 20, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20, 20, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 20, 20, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 40, 40, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 40, 40, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 40, 40, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 40, 40, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 200, 200, 128)    0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 200, 200, 128)     147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 200, 200, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200, 200, 128)     0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 200, 200, 3)       3459      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 200, 200, 3)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,275,715\n",
      "Trainable params: 2,274,179\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\Alberto Parenti\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0001, 0.5)  #Learning rate and momentum.\n",
    "\n",
    "# We are first going to build the discriminator and then train the generator as part of the combined model later\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "#Building and compiling our Generator, picking the loss function and optimizer\n",
    "#Since we are generating fake images there is no need to track any metrics.\n",
    "generator = build_generator(seed_size=100, channels=3)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "#Creating first the input latent space and then defining the generator  \n",
    "z = Input(shape=(100,))   #Our random vector to be used as an input\n",
    "\n",
    "#Storing the image as img\n",
    "img = generator(z)\n",
    "\n",
    "#Ensuring that when we combine Discriminator and Generator networks we only train the latter \n",
    "#This is to ensure that while the generator is being trained, the weights of the discriminator are not being adjusted\n",
    "#Note that this has no impact on the discriminator training above    \n",
    "discriminator.trainable = False  \n",
    "\n",
    "#Here our generator takes our image and classifies it as either real or fake  \n",
    "r_or_f = discriminator(img) \n",
    "\n",
    "\n",
    "#Here we combine the gen and disc models and define our loss function and optimizer. \n",
    "#To be sure, we are only training the generator here-\n",
    "#The objective in this step is for the generator to fool the discriminator  \n",
    "# The final combined model  (which is a stacked generator and discriminator) takes\n",
    "# noise (latent space) as an input => generates images => determines the validity of those images\n",
    "combined = Model(z, r_or_f)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "#Setting the model with the needed params\n",
    "train(array_data,epochs=150, batch_size=128, save_interval=500)\n",
    "\n",
    "#Saving model for future use\n",
    "generator.save('image_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "489273d9e700296fcf9e47b32fa24414c858b77e187d8a326e595227c9517409"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
